{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG and GPT-4o double checking for identifying properties\n",
        "\n",
        "Approach to identify the top match properties from a JSON file using RAG, and once it identifies them, ask GPT-4o again for a final check between the prompt's context and the top matched properties to get a final answer."
      ],
      "metadata": {
        "id": "prNmU3ayf0bf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvPm75DEKYfe"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install openai transformers torch scikit-learn faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "import openai\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from google.colab import userdata\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')  # Replace with your OpenAI API key\n",
        "\n",
        "# Initialize the OpenAI client with the API key\n",
        "client = OpenAI(api_key=openai_api_key)  # Pass the API key here\n",
        "\n",
        "# Function to use GPT-4o for processing the query\n",
        "def use_gpt_4o(query):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in generating SPARQL queries based on natural language queries.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Parse the following query and provide detailed properties: {query}\"}\n",
        "        ],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    # Print the entire response for inspection\n",
        "    print(response)\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Load the text embedding model and tokenizer\n",
        "model_name = \"TaylorAI/bge-micro-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to compute embeddings in batches\n",
        "def compute_embeddings(texts, batch_size=32, max_length=128):\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    all_embeddings = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask = [b.to(device) for b in batch]\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "            all_embeddings.append(embeddings)\n",
        "\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "# Load the Wikidata properties JSON file\n",
        "with open('/content/sample_data/props.json', 'r') as f:\n",
        "    wikidata_properties = json.load(f)\n",
        "\n",
        "# Extract property labels and aliases and compute their embeddings\n",
        "property_labels_and_aliases = []\n",
        "property_labels_map = {}  # Maps aliases to their respective labels\n",
        "for prop in wikidata_properties:\n",
        "    property_labels_and_aliases.append(prop['label'])\n",
        "    property_labels_map[prop['label']] = prop['label']\n",
        "    for alias in prop.get('aliases', []):\n",
        "        property_labels_and_aliases.append(alias)\n",
        "        property_labels_map[alias] = prop['label']\n",
        "\n",
        "property_embeddings = compute_embeddings(property_labels_and_aliases)\n",
        "\n",
        "# Build Faiss index\n",
        "d = property_embeddings.shape[1]  # Dimension of embeddings\n",
        "index = faiss.IndexFlatL2(d)  # L2 distance\n",
        "index.add(property_embeddings)  # Add embeddings to the index\n",
        "\n",
        "# Function to extract property from GPT-4o response\n",
        "def extract_property_from_response(response):\n",
        "    lines = response.split('\\n')\n",
        "    for line in lines:\n",
        "        if 'Predicate' in line or 'Property' in line:\n",
        "            return line.split(':')[-1].strip()\n",
        "    return None\n",
        "\n",
        "# Function to map a natural language question to Wikidata properties\n",
        "def map_question_to_properties(question, top_n=10):\n",
        "    gpt_response = use_gpt_4o(question)\n",
        "    print(f\"GPT-4o response: {gpt_response}\")\n",
        "\n",
        "    # Extract the relevant property from the GPT-4o response\n",
        "    relevant_property = extract_property_from_response(gpt_response)\n",
        "    if not relevant_property:\n",
        "        print(\"No relevant property found in the GPT-4o response.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Relevant property identified: {relevant_property}\")\n",
        "\n",
        "    # Compute the embedding for the relevant property\n",
        "    relevant_property_embedding = compute_embeddings([relevant_property])[0].reshape(1, -1)\n",
        "    distances, indices = index.search(relevant_property_embedding, top_n)\n",
        "    print(f\"indices: {indices}, distances: {distances}\")  # Debugging line\n",
        "\n",
        "    # Ensure valid indices and match with labels/aliases\n",
        "    valid_top_properties = []\n",
        "    seen_labels = set()  # To avoid duplicates\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        if idx < len(property_labels_and_aliases):\n",
        "            matched_label_or_alias = property_labels_and_aliases[idx]\n",
        "            actual_label = property_labels_map[matched_label_or_alias]\n",
        "            if actual_label not in seen_labels:\n",
        "                valid_top_properties.append((actual_label, dist))\n",
        "                seen_labels.add(actual_label)\n",
        "                if len(valid_top_properties) >= top_n:\n",
        "                    break\n",
        "\n",
        "    return valid_top_properties\n",
        "\n",
        "# Function to confirm the correct property with GPT-4o\n",
        "def confirm_property_with_gpt4o(question, top_properties):\n",
        "    properties_text = \"\\n\".join([f\"{i+1}. {prop[0]} (distance: {prop[1]:.4f})\" for i, prop in enumerate(top_properties)])\n",
        "    confirm_query = f\"\"\"\n",
        "    Based on the question \"{question}\", the following properties were identified as potential matches:\n",
        "\n",
        "    {properties_text}\n",
        "\n",
        "    Which one of these properties is the most appropriate considering the context of the question?\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in identifying the most appropriate property based on the context of a natural language question.\"},\n",
        "            {\"role\": \"user\", \"content\": confirm_query}\n",
        "        ],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    print(response)\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "m2_ZphTCfd81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "question = \"Where was Barack Obama born?\"\n",
        "top_properties = map_question_to_properties(question, top_n=10)\n",
        "print(\"Top matching Wikidata properties for the question:\")\n",
        "for label, distance in top_properties:\n",
        "    print(f\"{label}: {distance:.4f}\")\n",
        "\n",
        "confirmed_property = confirm_property_with_gpt4o(question, top_properties)\n",
        "print(f\"Confirmed property: {confirmed_property}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qfVmwHofi0r",
        "outputId": "3ac0b513-ebd4-4bda-d9d5-e134caccb272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-9WBv2B9J4Xn043YwowKaN8dUN18Da', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='To generate a SPARQL query for the natural language query \"Where was Barack Obama born?\", we need to identify the key components in the question. Here are the detailed properties:\\n\\n1. **Subject**: Barack Obama (the entity in question)\\n2. **Predicate**: Place of birth (the property we are seeking about the entity)\\n3. **Object**: The birth place (the result we are trying to retrieve)\\n\\nWe\\'ll assume we\\'re using a standard knowledge base like DBpedia, where entities and properties are well-defined and standardized.\\n\\n**Steps to Convert to SPARQL:**\\n\\n1. **Identify the URI for the Subject**:\\n   - In DBpedia, the URI for Barack Obama is usually `dbr:Barack_', role='assistant', function_call=None, tool_calls=None))], created=1717459340, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_319be4768e', usage=CompletionUsage(completion_tokens=150, prompt_tokens=42, total_tokens=192))\n",
            "GPT-4o response: To generate a SPARQL query for the natural language query \"Where was Barack Obama born?\", we need to identify the key components in the question. Here are the detailed properties:\n",
            "\n",
            "1. **Subject**: Barack Obama (the entity in question)\n",
            "2. **Predicate**: Place of birth (the property we are seeking about the entity)\n",
            "3. **Object**: The birth place (the result we are trying to retrieve)\n",
            "\n",
            "We'll assume we're using a standard knowledge base like DBpedia, where entities and properties are well-defined and standardized.\n",
            "\n",
            "**Steps to Convert to SPARQL:**\n",
            "\n",
            "1. **Identify the URI for the Subject**:\n",
            "   - In DBpedia, the URI for Barack Obama is usually `dbr:Barack_\n",
            "Relevant property identified: Place of birth (the property we are seeking about the entity)\n",
            "indices: [[28538 28541 28534 20561 28542 18709  2161 20288 29091 28543]], distances: [[23.173939 23.94736  25.326965 25.495052 25.565521 27.091183 27.641735\n",
            "  28.018026 28.057648 28.184294]]\n",
            "Top matching Wikidata properties for the question:\n",
            "place of birth: 23.1739\n",
            "category for people born here: 25.4951\n",
            "Wikidata item of this property: 27.0912\n",
            "Biografisches Handbuch â€“ Todesopfer der Grenzregime am Eisernen Vorhang ID: 27.6417\n",
            "birth rate: 28.0180\n",
            "property proposal discussion: 28.0576\n",
            "ChatCompletion(id='chatcmpl-9WBv4B9hyRTBJVHOLMTN8P1EIA2Hp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The most appropriate property considering the context of the question \"Where was Barack Obama born?\" is:\\n\\n1. **place of birth (distance: 23.1739)**\\n\\nThis property directly pertains to the location where someone was born, which is exactly what the question is asking for. The other properties are either irrelevant or tangentially related to the context.', role='assistant', function_call=None, tool_calls=None))], created=1717459342, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_319be4768e', usage=CompletionUsage(completion_tokens=72, prompt_tokens=178, total_tokens=250))\n",
            "Confirmed property: The most appropriate property considering the context of the question \"Where was Barack Obama born?\" is:\n",
            "\n",
            "1. **place of birth (distance: 23.1739)**\n",
            "\n",
            "This property directly pertains to the location where someone was born, which is exactly what the question is asking for. The other properties are either irrelevant or tangentially related to the context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding the ID of the property"
      ],
      "metadata": {
        "id": "srmq9ut4fbrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "import openai\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from google.colab import userdata\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')  # Replace with your OpenAI API key\n",
        "\n",
        "# Initialize the OpenAI client with the API key\n",
        "client = OpenAI(api_key=openai_api_key)  # Pass the API key here\n",
        "\n",
        "# Function to use GPT-4o for processing the query\n",
        "def use_gpt_4o(query):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in generating SPARQL queries based on natural language queries.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Parse the following query and provide detailed properties: {query}\"}\n",
        "        ],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    # Print the entire response for inspection\n",
        "    print(response)\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Load the text embedding model and tokenizer\n",
        "model_name = \"TaylorAI/bge-micro-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to compute embeddings in batches\n",
        "def compute_embeddings(texts, batch_size=32, max_length=128):\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    all_embeddings = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask = [b.to(device) for b in batch]\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "            all_embeddings.append(embeddings)\n",
        "\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "# Load the Wikidata properties JSON file\n",
        "with open('/content/sample_data/props.json', 'r') as f:\n",
        "    wikidata_properties = json.load(f)\n",
        "\n",
        "# Extract property labels and aliases and compute their embeddings\n",
        "property_labels_and_aliases = []\n",
        "property_labels_map = {}  # Maps aliases to their respective labels\n",
        "property_id_map = {}  # Maps labels to their respective IDs\n",
        "for prop in wikidata_properties:\n",
        "    property_labels_and_aliases.append(prop['label'])\n",
        "    property_labels_map[prop['label']] = prop['label']\n",
        "    property_id_map[prop['label']] = prop['id']\n",
        "    for alias in prop.get('aliases', []):\n",
        "        property_labels_and_aliases.append(alias)\n",
        "        property_labels_map[alias] = prop['label']\n",
        "\n",
        "property_embeddings = compute_embeddings(property_labels_and_aliases)\n",
        "\n",
        "# Build Faiss index\n",
        "d = property_embeddings.shape[1]  # Dimension of embeddings\n",
        "index = faiss.IndexFlatL2(d)  # L2 distance\n",
        "index.add(property_embeddings)  # Add embeddings to the index\n",
        "\n",
        "# Function to extract property from GPT-4o response\n",
        "def extract_property_from_response(response):\n",
        "    lines = response.split('\\n')\n",
        "    for line in lines:\n",
        "        if 'Predicate' in line or 'Property' in line:\n",
        "            return line.split(':')[-1].strip()\n",
        "    return None\n",
        "\n",
        "# Function to map a natural language question to Wikidata properties\n",
        "def map_question_to_properties(question, top_n=10):\n",
        "    gpt_response = use_gpt_4o(question)\n",
        "    print(f\"GPT-4o response: {gpt_response}\")\n",
        "\n",
        "    # Extract the relevant property from the GPT-4o response\n",
        "    relevant_property = extract_property_from_response(gpt_response)\n",
        "    if not relevant_property:\n",
        "        print(\"No relevant property found in the GPT-4o response.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Relevant property identified: {relevant_property}\")\n",
        "\n",
        "    # Compute the embedding for the relevant property\n",
        "    relevant_property_embedding = compute_embeddings([relevant_property])[0].reshape(1, -1)\n",
        "    distances, indices = index.search(relevant_property_embedding, top_n)\n",
        "    print(f\"indices: {indices}, distances: {distances}\")  # Debugging line\n",
        "\n",
        "    # Ensure valid indices and match with labels/aliases\n",
        "    valid_top_properties = []\n",
        "    seen_labels = set()  # To avoid duplicates\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        if idx < len(property_labels_and_aliases):\n",
        "            matched_label_or_alias = property_labels_and_aliases[idx]\n",
        "            actual_label = property_labels_map[matched_label_or_alias]\n",
        "            if actual_label not in seen_labels:\n",
        "                valid_top_properties.append((actual_label, dist))\n",
        "                seen_labels.add(actual_label)\n",
        "                if len(valid_top_properties) >= top_n:\n",
        "                    break\n",
        "\n",
        "    return valid_top_properties\n",
        "\n",
        "# Function to confirm the correct property with GPT-4o\n",
        "def confirm_property_with_gpt4o(question, top_properties):\n",
        "    properties_text = \"\\n\".join([f\"{i+1}. {prop[0]} (distance: {prop[1]:.4f})\" for i, prop in enumerate(top_properties)])\n",
        "    confirm_query = f\"\"\"\n",
        "    Based on the question \"{question}\", the following properties were identified as potential matches:\n",
        "\n",
        "    {properties_text}\n",
        "\n",
        "    Which one of these properties is the most appropriate considering the context of the question?\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in identifying the most appropriate property based on the context of a natural language question.\"},\n",
        "            {\"role\": \"user\", \"content\": confirm_query}\n",
        "        ],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    print(response)\n",
        "    confirmed_property_text = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Extract the property label from the GPT-4o confirmation response\n",
        "    confirmed_property_label = None\n",
        "    for prop in top_properties:\n",
        "        if prop[0] in confirmed_property_text:\n",
        "            confirmed_property_label = prop[0]\n",
        "            break\n",
        "\n",
        "    return confirmed_property_label"
      ],
      "metadata": {
        "id": "7AVKos3FLoZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "question = \"What is the birth date of Albert Einstein?\"\n",
        "top_properties = map_question_to_properties(question, top_n=10)\n",
        "print(\"Top matching Wikidata properties for the question:\")\n",
        "for label, distance in top_properties:\n",
        "    print(f\"{label}: {distance:.4f}\")\n",
        "\n",
        "confirmed_property_label = confirm_property_with_gpt4o(question, top_properties)\n",
        "\n",
        "# Lookup the property ID from the JSON file\n",
        "confirmed_property_id = property_id_map.get(confirmed_property_label, \"ID not found\")\n",
        "print(f\"Confirmed property: {confirmed_property_label}, ID: {confirmed_property_id}\")"
      ],
      "metadata": {
        "id": "ZZdfWV9VLu4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cac92fd-37d7-4558-9a2f-5fb5239608e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-9WBdkhKTkiD002IUgFIJiblGZOmVs', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='To generate a SPARQL query from a natural language question like \"What is the birth date of Albert Einstein?\", it\\'s important to:\\n\\n1. **Identify the subject:** Albert Einstein\\n2. **Identify the property being queried:** birth date\\n\\nGiven these components, we need the appropriate predicates and classes from the ontology to construct the SPARQL query. An example ontology that we could use is DBpedia. Below, I will break down the components and then generate the query.\\n\\n### Ontology Components\\n- **Subject (Entity):** `Albert Einstein`\\n- **Predicate (Property):** `birthDate` (In DBpedia, the corresponding property is `dbo:birthDate`)\\n\\n### SPARQL Query Components\\n1. **Selecting', role='assistant', function_call=None, tool_calls=None))], created=1717458268, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_319be4768e', usage=CompletionUsage(completion_tokens=150, prompt_tokens=45, total_tokens=195))\n",
            "GPT-4o response: To generate a SPARQL query from a natural language question like \"What is the birth date of Albert Einstein?\", it's important to:\n",
            "\n",
            "1. **Identify the subject:** Albert Einstein\n",
            "2. **Identify the property being queried:** birth date\n",
            "\n",
            "Given these components, we need the appropriate predicates and classes from the ontology to construct the SPARQL query. An example ontology that we could use is DBpedia. Below, I will break down the components and then generate the query.\n",
            "\n",
            "### Ontology Components\n",
            "- **Subject (Entity):** `Albert Einstein`\n",
            "- **Predicate (Property):** `birthDate` (In DBpedia, the corresponding property is `dbo:birthDate`)\n",
            "\n",
            "### SPARQL Query Components\n",
            "1. **Selecting\n",
            "Relevant property identified: birthDate`)\n",
            "indices: [[21687 21685 21683 20290 20288 20275 21686 20279 21688  9650]], distances: [[13.667583 18.23267  20.951645 21.53273  23.077217 23.263977 23.429222\n",
            "  23.670355 25.496513 26.671974]]\n",
            "Top matching Wikidata properties for the question:\n",
            "date of birth: 13.6676\n",
            "birthday: 21.5327\n",
            "birth rate: 23.0772\n",
            "birth name: 23.2640\n",
            "Klosterdatenbank ID: 26.6720\n",
            "ChatCompletion(id='chatcmpl-9WBdnOwXtzYFzsQf2On9mOIjB9rgF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The most appropriate property considering the context of the question \"What is the birth date of Albert Einstein?\" is:\\n\\n1. date of birth (distance: 13.6676)\\n\\nThis property directly corresponds to the inquiry about Albert Einstein\\'s birth date.', role='assistant', function_call=None, tool_calls=None))], created=1717458271, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_319be4768e', usage=CompletionUsage(completion_tokens=50, prompt_tokens=145, total_tokens=195))\n",
            "Confirmed property: date of birth, ID: P569\n"
          ]
        }
      ]
    }
  ]
}